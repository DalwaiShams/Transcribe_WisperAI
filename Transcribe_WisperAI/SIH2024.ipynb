{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"1inNvuWABccU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727066228448,"user_tz":-330,"elapsed":38719,"user":{"displayName":"DALWAI SHAMS MUKHTAR 211407","userId":"03968475020649611788"}},"outputId":"a71dffb4-c8c1-4c18-9a3c-48779f219049"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-pag3nvep\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-pag3nvep\n","  Resolved https://github.com/openai/whisper.git to commit 279133e3107392276dc509148da1f41bfb532c7e\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.26.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.4.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.5)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.5.0)\n","Collecting tiktoken (from openai-whisper==20231117)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting triton>=2.0.0 (from openai-whisper==20231117)\n","  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20231117) (3.16.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.32.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2024.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n","Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802819 sha256=e8231dad35cc182d827c3967152d8d2e2b3cf89a07d279ceeb6dc46cf311bb6f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-viyw3g8d/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n","Successfully built openai-whisper\n","Installing collected packages: triton, tiktoken, openai-whisper\n","Successfully installed openai-whisper-20231117 tiktoken-0.7.0 triton-3.0.0\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Ign:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n","Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,336 kB]\n","Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,153 kB]\n","Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,582 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,308 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,590 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,440 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,186 kB]\n","Fetched 21.9 MB in 4s (5,751 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n"]}],"source":["!pip install git+https://github.com/openai/whisper.git\n","!sudo apt update && sudo apt install ffmpeg"]},{"cell_type":"code","source":["ls\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LCbH33kID8V2","executionInfo":{"status":"ok","timestamp":1727066373802,"user_tz":-330,"elapsed":548,"user":{"displayName":"DALWAI SHAMS MUKHTAR 211407","userId":"03968475020649611788"}},"outputId":"167d222b-f75e-467d-d0fa-7c0b776bc97e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["SIH2024.ipynb    test_audio.mp3  test_audio.tsv  test_audio.vtt\n","test_audio.json  test_audio.srt  test_audio.txt\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bD7nGdtlLF51","executionInfo":{"status":"ok","timestamp":1727066327705,"user_tz":-330,"elapsed":23627,"user":{"displayName":"DALWAI SHAMS MUKHTAR 211407","userId":"03968475020649611788"}},"outputId":"187bec83-4f57-4f35-fab6-e46d65b15a4b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd drive/MyDrive/Transcribe_WisperAI"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pW1I3gOkDK7s","executionInfo":{"status":"ok","timestamp":1727066369804,"user_tz":-330,"elapsed":497,"user":{"displayName":"DALWAI SHAMS MUKHTAR 211407","userId":"03968475020649611788"}},"outputId":"d103a26d-aad1-47ed-b5be-4f115b0c6874"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Transcribe_WisperAI\n"]}]},{"cell_type":"code","source":["!whisper \"test_audio.mp3\" --model medium.en"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CF2vYG2bDSia","executionInfo":{"status":"ok","timestamp":1727066640018,"user_tz":-330,"elapsed":58606,"user":{"displayName":"DALWAI SHAMS MUKHTAR 211407","userId":"03968475020649611788"}},"outputId":"e6035e18-d7c8-4f01-f7c3-0e4f4642e7a1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["100%|█████████████████████████████████████| 1.42G/1.42G [00:19<00:00, 77.3MiB/s]\n","/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(fp, map_location=device)\n","[00:00.000 --> 00:05.440]  Hello everyone, I'm Shams Del Wai, a proud member of Metamorphs, and today I'm thrilled to present\n","[00:05.440 --> 00:10.320]  one of the most exciting projects we've been working on, a solution that converts any audio\n","[00:10.320 --> 00:16.800]  file into a text file. Our team, Metamorphs, is dedicated to creating innovative and transformative\n","[00:16.800 --> 00:23.440]  technologies, and this project is no exception. In the digital era, the need to convert audio data\n","[00:23.440 --> 00:28.480]  into readable text is more critical than ever. Whether you're transcribing meetings, converting\n","[00:28.480 --> 00:34.240]  podcasts, or processing spoken content for analysis, transforming audio into text brings\n","[00:34.240 --> 00:40.240]  efficiency and accessibility. This project was born out of the desire to make audio content\n","[00:40.240 --> 00:46.720]  easier to manipulate, analyze, and repurpose across various domains. From creating searchable\n","[00:46.720 --> 00:51.760]  archives of interviews and lectures to enabling real-time transcription in education or business\n","[00:51.760 --> 00:55.760]  settings, the scope of this technology is far-reaching.\n"]}]},{"cell_type":"code","source":["file_path = '/content/drive/My Drive/Transcribe_WisperAI/test_audio.txt'\n","\n","with open(file_path, 'r') as file:\n","    content = file.read()\n","\n","print(content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VcbOzkzTFcaf","executionInfo":{"status":"ok","timestamp":1727066678002,"user_tz":-330,"elapsed":767,"user":{"displayName":"DALWAI SHAMS MUKHTAR 211407","userId":"03968475020649611788"}},"outputId":"cdcd7148-1b1d-4c05-d8eb-d06ed40bd58a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello everyone, I'm Shams Del Wai, a proud member of Metamorphs, and today I'm thrilled to present\n","one of the most exciting projects we've been working on, a solution that converts any audio\n","file into a text file. Our team, Metamorphs, is dedicated to creating innovative and transformative\n","technologies, and this project is no exception. In the digital era, the need to convert audio data\n","into readable text is more critical than ever. Whether you're transcribing meetings, converting\n","podcasts, or processing spoken content for analysis, transforming audio into text brings\n","efficiency and accessibility. This project was born out of the desire to make audio content\n","easier to manipulate, analyze, and repurpose across various domains. From creating searchable\n","archives of interviews and lectures to enabling real-time transcription in education or business\n","settings, the scope of this technology is far-reaching.\n","\n"]}]},{"cell_type":"markdown","source":["AUR OPTIONS DEKHNE K LIYE JUST RUN **!whisper -h**"],"metadata":{"id":"nM2nQXnVGSPd"}},{"cell_type":"code","source":[" !whisper -h"],"metadata":{"id":"GxnNEh-fFko2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727066707152,"user_tz":-330,"elapsed":2766,"user":{"displayName":"DALWAI SHAMS MUKHTAR 211407","userId":"03968475020649611788"}},"outputId":"a116f7b2-e2f1-49bf-84fe-703d3ab8ce88"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: whisper [-h] [--model MODEL] [--model_dir MODEL_DIR] [--device DEVICE]\n","               [--output_dir OUTPUT_DIR] [--output_format {txt,vtt,srt,tsv,json,all}]\n","               [--verbose VERBOSE] [--task {transcribe,translate}]\n","               [--language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}]\n","               [--temperature TEMPERATURE] [--best_of BEST_OF] [--beam_size BEAM_SIZE]\n","               [--patience PATIENCE] [--length_penalty LENGTH_PENALTY]\n","               [--suppress_tokens SUPPRESS_TOKENS] [--initial_prompt INITIAL_PROMPT]\n","               [--condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT] [--fp16 FP16]\n","               [--temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK]\n","               [--compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD]\n","               [--logprob_threshold LOGPROB_THRESHOLD] [--no_speech_threshold NO_SPEECH_THRESHOLD]\n","               [--word_timestamps WORD_TIMESTAMPS] [--prepend_punctuations PREPEND_PUNCTUATIONS]\n","               [--append_punctuations APPEND_PUNCTUATIONS] [--highlight_words HIGHLIGHT_WORDS]\n","               [--max_line_width MAX_LINE_WIDTH] [--max_line_count MAX_LINE_COUNT]\n","               [--max_words_per_line MAX_WORDS_PER_LINE] [--threads THREADS]\n","               [--clip_timestamps CLIP_TIMESTAMPS]\n","               [--hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD]\n","               audio [audio ...]\n","\n","positional arguments:\n","  audio                 audio file(s) to transcribe\n","\n","options:\n","  -h, --help            show this help message and exit\n","  --model MODEL         name of the Whisper model to use (default: small)\n","  --model_dir MODEL_DIR\n","                        the path to save model files; uses ~/.cache/whisper by default (default:\n","                        None)\n","  --device DEVICE       device to use for PyTorch inference (default: cuda)\n","  --output_dir OUTPUT_DIR, -o OUTPUT_DIR\n","                        directory to save the outputs (default: .)\n","  --output_format {txt,vtt,srt,tsv,json,all}, -f {txt,vtt,srt,tsv,json,all}\n","                        format of the output file; if not specified, all available formats will be\n","                        produced (default: all)\n","  --verbose VERBOSE     whether to print out the progress and debug messages (default: True)\n","  --task {transcribe,translate}\n","                        whether to perform X->X speech recognition ('transcribe') or X->English\n","                        translation ('translate') (default: transcribe)\n","  --language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}\n","                        language spoken in the audio, specify None to perform language detection\n","                        (default: None)\n","  --temperature TEMPERATURE\n","                        temperature to use for sampling (default: 0)\n","  --best_of BEST_OF     number of candidates when sampling with non-zero temperature (default: 5)\n","  --beam_size BEAM_SIZE\n","                        number of beams in beam search, only applicable when temperature is zero\n","                        (default: 5)\n","  --patience PATIENCE   optional patience value to use in beam decoding, as in\n","                        https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to\n","                        conventional beam search (default: None)\n","  --length_penalty LENGTH_PENALTY\n","                        optional token length penalty coefficient (alpha) as in\n","                        https://arxiv.org/abs/1609.08144, uses simple length normalization by\n","                        default (default: None)\n","  --suppress_tokens SUPPRESS_TOKENS\n","                        comma-separated list of token ids to suppress during sampling; '-1' will\n","                        suppress most special characters except common punctuations (default: -1)\n","  --initial_prompt INITIAL_PROMPT\n","                        optional text to provide as a prompt for the first window. (default: None)\n","  --condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT\n","                        if True, provide the previous output of the model as a prompt for the next\n","                        window; disabling may make the text inconsistent across windows, but the\n","                        model becomes less prone to getting stuck in a failure loop (default:\n","                        True)\n","  --fp16 FP16           whether to perform inference in fp16; True by default (default: True)\n","  --temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK\n","                        temperature to increase when falling back when the decoding fails to meet\n","                        either of the thresholds below (default: 0.2)\n","  --compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD\n","                        if the gzip compression ratio is higher than this value, treat the\n","                        decoding as failed (default: 2.4)\n","  --logprob_threshold LOGPROB_THRESHOLD\n","                        if the average log probability is lower than this value, treat the\n","                        decoding as failed (default: -1.0)\n","  --no_speech_threshold NO_SPEECH_THRESHOLD\n","                        if the probability of the <|nospeech|> token is higher than this value AND\n","                        the decoding has failed due to `logprob_threshold`, consider the segment\n","                        as silence (default: 0.6)\n","  --word_timestamps WORD_TIMESTAMPS\n","                        (experimental) extract word-level timestamps and refine the results based\n","                        on them (default: False)\n","  --prepend_punctuations PREPEND_PUNCTUATIONS\n","                        if word_timestamps is True, merge these punctuation symbols with the next\n","                        word (default: \"'“¿([{-)\n","  --append_punctuations APPEND_PUNCTUATIONS\n","                        if word_timestamps is True, merge these punctuation symbols with the\n","                        previous word (default: \"'.。,，!！?？:：”)]}、)\n","  --highlight_words HIGHLIGHT_WORDS\n","                        (requires --word_timestamps True) underline each word as it is spoken in\n","                        srt and vtt (default: False)\n","  --max_line_width MAX_LINE_WIDTH\n","                        (requires --word_timestamps True) the maximum number of characters in a\n","                        line before breaking the line (default: None)\n","  --max_line_count MAX_LINE_COUNT\n","                        (requires --word_timestamps True) the maximum number of lines in a segment\n","                        (default: None)\n","  --max_words_per_line MAX_WORDS_PER_LINE\n","                        (requires --word_timestamps True, no effect with --max_line_width) the\n","                        maximum number of words in a segment (default: None)\n","  --threads THREADS     number of threads used by torch for CPU inference; supercedes\n","                        MKL_NUM_THREADS/OMP_NUM_THREADS (default: 0)\n","  --clip_timestamps CLIP_TIMESTAMPS\n","                        comma-separated list start,end,start,end,... timestamps (in seconds) of\n","                        clips to process, where the last end timestamp defaults to the end of the\n","                        file (default: 0)\n","  --hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD\n","                        (requires --word_timestamps True) skip silent periods longer than this\n","                        threshold (in seconds) when a possible hallucination is detected (default:\n","                        None)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"oefPHW-wMocG"},"execution_count":null,"outputs":[]}]}